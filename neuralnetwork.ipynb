{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<최적 신경망 구성의 단계적 절차>\n",
    "\n",
    "1. 매개변수 설정(입력노드, 은닉층 노드, 출력층 노드, 학습률, 학습횟수(epoch))\n",
    "\n",
    "\n",
    "2. 학습용 train과 평가용 test 데이터 셋을 추출\n",
    "\n",
    "\n",
    "3. 다중 글자를 분류하여 학습하고 테스트 하는 것이기 때문에 다중 클래스 분류를 위해서 원-핫인코딩 방식을 채택\n",
    "\n",
    "\n",
    "4. 순전파 은닉층에서 사용 할 활성화 함수 정의(시그모이드 함수, 하이퍼볼릭 탄젠트, Relu)\n",
    "\n",
    "\n",
    "5. 역전파 은닉층에서 사용 할 활성화 함수 미분 정의\n",
    "\n",
    "\n",
    "6. 출력층에서 사용 할 활성화 함수(소프트 맥스=> 다중 클래스 분류에서 사용하기 용이함)\n",
    "\n",
    "\n",
    "7. 학습의 정확도와 평가의 정확도를 알기 위해서 정확도 함수를 정의\n",
    "\n",
    "\n",
    "8. 신경망을 구성하여 순전파로 오차를 구하고, 이 오차의 미분을 통하여 기울기를 구함. 이를 통해 가중치와 편향을 업데이트 하여 학습 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 구성하면 서 필요한 라이브러리들 import\n",
    "csv파일을 읽어 들일 수 있는 csv\n",
    "현재 디렉토리에 있는 파일에서 글자데이터를 찾고 train과 test에서 필요한 파일들을 찾을 수 있는 os\n",
    "신경망의 행렬 계산을 쉽게 해줄 수 있는 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 신경망에서 필수적으로 필요한 각각의 매개변수들을 정의함.\n",
    "\n",
    "1.2 csv파일로 만든 train 데이터 셋과 test 데이터 셋을 메모리에 로드 할 수 있게 하는 함수를 정의 함.(입력값: 픽셀 데이터, 출력 값: 레이블)로 구성됨.\n",
    "\n",
    "1.3 CSV 파일의 첫 번째 열은 클래스 레이블로 저장되어 있고 \n",
    "나머지 열은 각 글자를 표현하는 픽셀 데이터로 구성되어 있어 다중 클래스 분류에서 사용하기 용이하게 함.\n",
    "예를 들어 \"가\"라는 글자는 [1, 0, 0, ..., 0] 형태로 변환 이를 통하여 소프트맥스 활성화 함수를 사용하고 예측 확률이 가장 높은 클래스를 선택하기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 하이퍼파라미터 설정\n",
    "input_size = 4096           # 입력 크기: 64x64 이미지의 총 픽셀 수 (4096)\n",
    "hidden_layers = [256,240,220]  # 은닉층 노드 수\n",
    "output_size = 111            # 출력층 노드 수: 분류할 클래스 수\n",
    "learning_rate = 0.0001       # 학습률: 가중치 업데이트의 크기\n",
    "epochs = 350                  # 학습 데이터 전체를 얼마나 반복할지\n",
    "\n",
    "#데이터 로드 함수 \n",
    "def load_data(sub_dir, file_name, output_size):\n",
    "    current_dir = os.getcwd()  # 현재 작업 디렉토리 경로 가져오기\n",
    "    file_path = os.path.join(current_dir, '한글글자데이터', sub_dir, file_name)   # 데이터 파일 경로\n",
    "\n",
    "    pixel_data = [] # 픽셀 데이터\n",
    "    labels = [] # 레이블(정답) 데이터\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # 문자열을 정수로 변환\n",
    "            label = int(row[0].strip()) - 1  # 첫 번째 열은 레이블 (0~110)\n",
    "            row_data = [float(value) / 255.0 for value in row[1:]]  # 픽셀 데이터 정규화 (0~255 -> 0~1)\n",
    "            pixel_data.append(row_data)  # 픽셀 데이터를 리스트에 추가\n",
    "\n",
    "            # 원핫 인코딩 생성\n",
    "            one_hot_label = [0] * output_size  # 출력 크기만큼 0으로 채운 리스트 생성\n",
    "            one_hot_label[label] = 1  # 해당 레이블 인덱스를 1로 설정\n",
    "            labels.append(one_hot_label)  # 원핫 인코딩된 레이블을 리스트에 추가\n",
    "\n",
    "    # 데이터를 numpy 배열로 변환하여 반환\n",
    "    return np.array(pixel_data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의 시간 때 배웠던 활성화 함수들을 정의하였다.\n",
    "1. 시그모이드 함수\n",
    "2. 하이퍼볼릭 탄젠트 함수\n",
    "3. Relu 함수\n",
    "4. 다중 클래스 분류를 위한 소프트 맥스 함수(출력층에서만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"활성화함수 정의\"\n",
    "\n",
    "#시그모이드 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # 시그모이드 함수: 1 / (1 + e^(-x))\n",
    "\n",
    "# Relu(순전파 사용)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)  # ReLU 함수: 입력이 음수이면 0, 양수이면 그대로 출력\n",
    "\n",
    "# tanh(순전파 사용)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)  # tanh 함수: (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "\n",
    "#시그모이드 미분(역전파 사용)\n",
    "def sigmoid_derivative(x):  \n",
    "    return sigmoid(x) * (1 - sigmoid(x))  # 시그모이드 미분: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Relu 미분(역전파 사용)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)  # ReLU 미분: 입력이 양수이면 1, 음수이면 0\n",
    "\n",
    "# tanh 미분(역전파 사용)\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2  # tanh 미분: 1 - tanh(x)^2\n",
    "\n",
    "# 출력층 활성화함수 정의\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)  # 입력값 x에 대한 지수 함수 계산\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # softmax: 확률로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 모델의 순전파를 통하여 오차를 구하기 위해서 평균제곱 오차를 사용하였다.\n",
    "평균 제곱 오차의 계산은 (예측값-실제값)^2와 같이 계산이 되고, 이를 통하여 오차를 계산하고 역전파에서 가중치와 편향을 업데이트한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"손실함수 정의\"\n",
    "\n",
    "# 평균제곱오차\n",
    "def MSE(y_true, y_pred):    # 실제값과 예측값을 사용하여 평균제곱오차 계산\n",
    "    return np.mean((y_true-y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 정확도를 통하여 학습의 정확도와 평가용 테스트 셋을 준비한(test데이터)의 정확도를 통하여 모델이 얼마나 학습을 잘하고 있는지 정확도 함수를 정의하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"정확도 계산\"\n",
    "\n",
    "# 정확도 계산 함수 정의\n",
    "def accuracy(y_true, y_pred):   # 정답과 예측값을 받아서 정확도 계산\n",
    "    if y_true.ndim > 1:      \n",
    "        y_true = np.argmax(y_true, axis=1)  #원한 인코딩 정수 변환\n",
    "    if y_pred.ndim > 1:  \n",
    "        y_pred = np.argmax(y_pred, axis=1)  # 원핫 인코딩 정수 변환\n",
    "    correct = np.sum(y_true == y_pred)  # 정답과 예측이 일치하는 개수 계산\n",
    "    return correct / len(y_true)  # 정확도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 구성\n",
    "\n",
    "1. 순전파\n",
    "입력층(4096)의 데이터를 순전파하여 출력층(111)의 예측값을 계산하고 이를 실제값과 비교하여 손실 함수로 오차를 계산함\n",
    "\n",
    "2. 역전파\n",
    "순전파에서 계산된 오차를 토대로 손실 함수의 기울기를 계산하고 가중치와 편향을 조정 함.\n",
    "\n",
    "3. 학습\n",
    "경사하강법을 통한 가중치와 편향의 업데이트가 여러번의 에폭(학습과정)을 통하여 모델을 조정해 나감으로써 모델의 예측 성능을 점진적으로 높임.\n",
    "(학습 과정에서 경사하강법으로 인한 기울기 소실 문제가 발생하는 것을 대비하여 가중치의 he초기화를 통하여 기울기 소실 방지)\n",
    "\n",
    "4. 평가\n",
    "평가용 데이터 셋으로 만들어진 학습 데이터에서는 사용되지 않은 폰트들과 노이즈가 섞인 데이터를 학습 데이터와 비교하며 모델이 범용성이 있는지를 보임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 클래스 정의\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, learning_rate):\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.weights = [] \n",
    "        self.biases = []  \n",
    "        \n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]  # 입력, 은닉, 출력층 노드 수\n",
    "        \n",
    "        # 각 레이어 가중치와 편향 초기화\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2.0 / layer_sizes[i])  # 심층 신경망에서 기울기 소실 방지를 위한 he 초기화\n",
    "            bias = np.zeros((1, layer_sizes[i + 1]))  # 편향 초기화\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    # 순전파\n",
    "    def forward(self, x, training=True):\n",
    "        self.activations = [x]  # 입력 데이터를 첫 번째 활성화 값으로 설정\n",
    "        # 은닉층 활성화 함수(Relu) 계산\n",
    "        for i in range(len(self.weights) - 1):  # 은닉층 수만큼 반복\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]  # 업데이트 된 가중치와 편향을 통하여 활성화 함수 계산 \n",
    "            a = relu(z)  # 활성화 함수 적용\n",
    "            self.activations.append(a)\n",
    "        # 출력층 활성화 함수(소프트 맥스)계산\n",
    "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        a = softmax(z)  # 소프트맥스 활성화 함수 적용\n",
    "        self.activations.append(a)\n",
    "        return self.activations[-1] # 출력층 활성화 값 반환\n",
    "\n",
    "    # 역전파\n",
    "    def backward(self, y_true):\n",
    "        # 출력층 오차\n",
    "        deltas = [self.activations[-1] - y_true]\n",
    "        \n",
    "        # 은닉층에서의 오차 전파(출력층-> 은닉층 , 은닉층->입력층)\n",
    "        for i in reversed(range(len(self.weights) - 1)):    \n",
    "            delta = deltas[-1].dot(self.weights[i + 1].T) * relu_derivative(self.activations[i + 1])    # 은닉층 오차 계산\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()  # 순서를 원래대로 변경\n",
    "        \n",
    "        # 가중치 및 편향 업데이트\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * self.activations[i].T.dot(deltas[i])    # 가중치 업데이트  \n",
    "            self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True) # 편향 업데이트\n",
    "\n",
    "\n",
    "    # 학습\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # 클래스별 데이터 인덱스 생성\n",
    "        class_indices = {label: np.where(np.argmax(y, axis=1) == label)[0] for label in range(y.shape[1])}\n",
    "\n",
    "        for epoch in range(epochs): \n",
    "            if (epoch + 1) % 10 == 0:   #주기적 모델 평가\n",
    "                print(\"모델 범용성 평가\")\n",
    "                self.evaluate(x, y) \n",
    "\n",
    "            total_loss = 0  # 총 손실 초기화\n",
    "            correct_predictions = 0  # 정확한 예측 수 초기화\n",
    "\n",
    "            # 클래스별 학습 루프\n",
    "            for class_label, indices in class_indices.items():\n",
    "                np.random.shuffle(indices)  # 클래스 데이터를 섞어서 순차적으로 학습\n",
    "                class_x = x[indices]    #클래스 픽셀 데이터\n",
    "                class_y = y[indices]    #클래스 레이블 데이터\n",
    "\n",
    "                # 순전파\n",
    "                output = self.forward(class_x, training=True)   \n",
    "                loss = MSE(class_y, output) #실제값과 예측값의 손실 계산\n",
    "                total_loss += loss  #총 손실 누적\n",
    "\n",
    "                # 정확도 계산\n",
    "                predictions = np.argmax(output, axis=1) #예측값\n",
    "                true_labels = np.argmax(class_y, axis=1)    #실제값\n",
    "                correct_predictions += np.sum(predictions == true_labels)   #정확한 예측 수 계산\n",
    "\n",
    "                # 역전파\n",
    "                self.backward(class_y)  \n",
    "\n",
    "            # 평균 손실 및 학습 정확도 계산\n",
    "            average_loss = total_loss / len(x)  # 전체 데이터 크기로 나눔\n",
    "            train_accuracy = correct_predictions / len(x) * 100\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {average_loss}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # 평가 함수\n",
    "    def evaluate(self, x, y):\n",
    "        test_output = self.forward(x, training=False)   #테스트 데이터에 대한 예측값\n",
    "        test_loss = MSE(y, test_output) #테스트 데이터에 대한 손실 계산\n",
    "        test_predictions = np.argmax(test_output, axis=1)   #테스트 데이터 예측값\n",
    "        test_true_labels = np.argmax(y, axis=1) #테스트 데이터 실제값\n",
    "        test_accuracy = accuracy(test_true_labels, test_predictions)    #테스트 데이터 정확도 계산\n",
    "\n",
    "        print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "        return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습용 데이터셋 csv와 평가용 데이터셋 csv를 불러들여오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_data, train_labels = load_data('train', 'train_data.csv', output_size)\n",
    "test_data, test_labels = load_data('test', 'test_data.csv', output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 객체를 생성하여 학습을 실행하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 객체 생성\n",
    "nn = NeuralNetwork(input_size, hidden_layers, output_size, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350, Train Loss: 0.0009003244491313524, Train Accuracy: 0.45%\n",
      "Epoch 2/350, Train Loss: 0.0008946354926216029, Train Accuracy: 0.54%\n",
      "Epoch 3/350, Train Loss: 0.0008935639008873349, Train Accuracy: 1.53%\n",
      "Epoch 4/350, Train Loss: 0.0008930877339833101, Train Accuracy: 1.44%\n",
      "Epoch 5/350, Train Loss: 0.0008928110410662944, Train Accuracy: 1.80%\n",
      "Epoch 6/350, Train Loss: 0.0008922563790884084, Train Accuracy: 2.25%\n",
      "Epoch 7/350, Train Loss: 0.0008918872830571592, Train Accuracy: 2.70%\n",
      "Epoch 8/350, Train Loss: 0.0008914172081411745, Train Accuracy: 3.60%\n",
      "Epoch 9/350, Train Loss: 0.0008909300453326895, Train Accuracy: 3.96%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.00889680328072672, Test Accuracy: 6.22%\n",
      "Epoch 10/350, Train Loss: 0.0008903832587163656, Train Accuracy: 5.32%\n",
      "Epoch 11/350, Train Loss: 0.0008898543150183026, Train Accuracy: 5.50%\n",
      "Epoch 12/350, Train Loss: 0.0008892496823108471, Train Accuracy: 5.86%\n",
      "Epoch 13/350, Train Loss: 0.0008885957590184105, Train Accuracy: 8.38%\n",
      "Epoch 14/350, Train Loss: 0.0008878560722169479, Train Accuracy: 8.11%\n",
      "Epoch 15/350, Train Loss: 0.0008872154433122374, Train Accuracy: 7.93%\n",
      "Epoch 16/350, Train Loss: 0.0008865102118944053, Train Accuracy: 8.56%\n",
      "Epoch 17/350, Train Loss: 0.0008854647845014669, Train Accuracy: 8.74%\n",
      "Epoch 18/350, Train Loss: 0.0008844146263937465, Train Accuracy: 9.37%\n",
      "Epoch 19/350, Train Loss: 0.0008834866899648085, Train Accuracy: 9.55%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008845799439533444, Test Accuracy: 7.39%\n",
      "Epoch 20/350, Train Loss: 0.0008822946054258177, Train Accuracy: 9.55%\n",
      "Epoch 21/350, Train Loss: 0.0008807967322739508, Train Accuracy: 9.73%\n",
      "Epoch 22/350, Train Loss: 0.0008791849007768729, Train Accuracy: 10.45%\n",
      "Epoch 23/350, Train Loss: 0.0008780632442972517, Train Accuracy: 9.46%\n",
      "Epoch 24/350, Train Loss: 0.0008762409921077422, Train Accuracy: 10.36%\n",
      "Epoch 25/350, Train Loss: 0.0008746640970523454, Train Accuracy: 10.63%\n",
      "Epoch 26/350, Train Loss: 0.0008724533180313801, Train Accuracy: 10.63%\n",
      "Epoch 27/350, Train Loss: 0.0008702652748330626, Train Accuracy: 11.17%\n",
      "Epoch 28/350, Train Loss: 0.0008671728357395766, Train Accuracy: 12.34%\n",
      "Epoch 29/350, Train Loss: 0.0008643570983958726, Train Accuracy: 13.15%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.00876833741548433, Test Accuracy: 10.00%\n",
      "Epoch 30/350, Train Loss: 0.0008614628303898902, Train Accuracy: 13.42%\n",
      "Epoch 31/350, Train Loss: 0.0008581472187308848, Train Accuracy: 14.23%\n",
      "Epoch 32/350, Train Loss: 0.0008536877097832093, Train Accuracy: 14.59%\n",
      "Epoch 33/350, Train Loss: 0.0008498713617071109, Train Accuracy: 14.86%\n",
      "Epoch 34/350, Train Loss: 0.0008445482408453877, Train Accuracy: 15.86%\n",
      "Epoch 35/350, Train Loss: 0.0008385460741759961, Train Accuracy: 16.58%\n",
      "Epoch 36/350, Train Loss: 0.0008340372392689523, Train Accuracy: 17.48%\n",
      "Epoch 37/350, Train Loss: 0.0008276747377688423, Train Accuracy: 18.02%\n",
      "Epoch 38/350, Train Loss: 0.0008213594026031627, Train Accuracy: 19.46%\n",
      "Epoch 39/350, Train Loss: 0.0008138670321761255, Train Accuracy: 20.45%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008686449458987933, Test Accuracy: 12.07%\n",
      "Epoch 40/350, Train Loss: 0.0008052113116061323, Train Accuracy: 21.71%\n",
      "Epoch 41/350, Train Loss: 0.0007966740152628237, Train Accuracy: 23.42%\n",
      "Epoch 42/350, Train Loss: 0.0007876112214028034, Train Accuracy: 24.59%\n",
      "Epoch 43/350, Train Loss: 0.0007778983840998218, Train Accuracy: 26.22%\n",
      "Epoch 44/350, Train Loss: 0.0007674650701331591, Train Accuracy: 28.38%\n",
      "Epoch 45/350, Train Loss: 0.0007564218908306598, Train Accuracy: 31.08%\n",
      "Epoch 46/350, Train Loss: 0.000745879498831857, Train Accuracy: 32.34%\n",
      "Epoch 47/350, Train Loss: 0.0007335695794587789, Train Accuracy: 34.32%\n",
      "Epoch 48/350, Train Loss: 0.0007226400967851594, Train Accuracy: 35.59%\n",
      "Epoch 49/350, Train Loss: 0.0007111769545969895, Train Accuracy: 36.76%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008767475423688215, Test Accuracy: 15.95%\n",
      "Epoch 50/350, Train Loss: 0.0006995690943299367, Train Accuracy: 38.92%\n",
      "Epoch 51/350, Train Loss: 0.0006890181283027892, Train Accuracy: 39.82%\n",
      "Epoch 52/350, Train Loss: 0.0006781260953019838, Train Accuracy: 41.26%\n",
      "Epoch 53/350, Train Loss: 0.0006653886417501126, Train Accuracy: 43.33%\n",
      "Epoch 54/350, Train Loss: 0.0006537820107512662, Train Accuracy: 44.23%\n",
      "Epoch 55/350, Train Loss: 0.0006432559771582652, Train Accuracy: 45.59%\n",
      "Epoch 56/350, Train Loss: 0.0006333569478261357, Train Accuracy: 46.58%\n",
      "Epoch 57/350, Train Loss: 0.0006228240472876242, Train Accuracy: 47.57%\n",
      "Epoch 58/350, Train Loss: 0.0006117075841435011, Train Accuracy: 49.19%\n",
      "Epoch 59/350, Train Loss: 0.0006029814306898013, Train Accuracy: 50.27%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008774483409843626, Test Accuracy: 20.27%\n",
      "Epoch 60/350, Train Loss: 0.0005925922255213362, Train Accuracy: 50.54%\n",
      "Epoch 61/350, Train Loss: 0.0005801274919765687, Train Accuracy: 51.44%\n",
      "Epoch 62/350, Train Loss: 0.0005707674671247391, Train Accuracy: 52.34%\n",
      "Epoch 63/350, Train Loss: 0.0005605587831614124, Train Accuracy: 53.60%\n",
      "Epoch 64/350, Train Loss: 0.000551200664479914, Train Accuracy: 54.95%\n",
      "Epoch 65/350, Train Loss: 0.0005407348593129689, Train Accuracy: 55.77%\n",
      "Epoch 66/350, Train Loss: 0.0005301416057086863, Train Accuracy: 57.03%\n",
      "Epoch 67/350, Train Loss: 0.0005196613412204369, Train Accuracy: 57.30%\n",
      "Epoch 68/350, Train Loss: 0.0005100029774913903, Train Accuracy: 57.84%\n",
      "Epoch 69/350, Train Loss: 0.0005009359325131385, Train Accuracy: 58.20%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008674422212019193, Test Accuracy: 24.59%\n",
      "Epoch 70/350, Train Loss: 0.0004905467090782567, Train Accuracy: 59.37%\n",
      "Epoch 71/350, Train Loss: 0.00048100464357707823, Train Accuracy: 59.91%\n",
      "Epoch 72/350, Train Loss: 0.0004736238662383074, Train Accuracy: 60.81%\n",
      "Epoch 73/350, Train Loss: 0.0004645953837430571, Train Accuracy: 61.80%\n",
      "Epoch 74/350, Train Loss: 0.00045623255872406007, Train Accuracy: 63.15%\n",
      "Epoch 75/350, Train Loss: 0.0004470854338989658, Train Accuracy: 63.78%\n",
      "Epoch 76/350, Train Loss: 0.00043745086368260756, Train Accuracy: 65.41%\n",
      "Epoch 77/350, Train Loss: 0.0004289365165127765, Train Accuracy: 65.95%\n",
      "Epoch 78/350, Train Loss: 0.00042154227237434103, Train Accuracy: 67.30%\n",
      "Epoch 79/350, Train Loss: 0.0004147686521878035, Train Accuracy: 67.48%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.00863502235692775, Test Accuracy: 27.84%\n",
      "Epoch 80/350, Train Loss: 0.0004060505395585366, Train Accuracy: 68.74%\n",
      "Epoch 81/350, Train Loss: 0.0003981598995291824, Train Accuracy: 69.46%\n",
      "Epoch 82/350, Train Loss: 0.0003908978624721367, Train Accuracy: 70.00%\n",
      "Epoch 83/350, Train Loss: 0.0003844057905626537, Train Accuracy: 70.54%\n",
      "Epoch 84/350, Train Loss: 0.0003767786788287109, Train Accuracy: 71.44%\n",
      "Epoch 85/350, Train Loss: 0.00037013962763760564, Train Accuracy: 71.98%\n",
      "Epoch 86/350, Train Loss: 0.0003635132748962121, Train Accuracy: 72.61%\n",
      "Epoch 87/350, Train Loss: 0.0003557990558139849, Train Accuracy: 73.15%\n",
      "Epoch 88/350, Train Loss: 0.0003506290721920559, Train Accuracy: 73.78%\n",
      "Epoch 89/350, Train Loss: 0.0003441360818141369, Train Accuracy: 74.14%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.00857183766485399, Test Accuracy: 29.37%\n",
      "Epoch 90/350, Train Loss: 0.0003400718830524436, Train Accuracy: 74.41%\n",
      "Epoch 91/350, Train Loss: 0.00033468431099478274, Train Accuracy: 75.68%\n",
      "Epoch 92/350, Train Loss: 0.0003285701419479711, Train Accuracy: 76.13%\n",
      "Epoch 93/350, Train Loss: 0.0003243003673558279, Train Accuracy: 76.94%\n",
      "Epoch 94/350, Train Loss: 0.0003177956024423546, Train Accuracy: 77.48%\n",
      "Epoch 95/350, Train Loss: 0.0003121546177632946, Train Accuracy: 77.66%\n",
      "Epoch 96/350, Train Loss: 0.0003073073017316526, Train Accuracy: 78.29%\n",
      "Epoch 97/350, Train Loss: 0.0003014854080995589, Train Accuracy: 79.28%\n",
      "Epoch 98/350, Train Loss: 0.00029600004144413, Train Accuracy: 79.64%\n",
      "Epoch 99/350, Train Loss: 0.0002907369421483688, Train Accuracy: 79.82%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008601881129480586, Test Accuracy: 31.44%\n",
      "Epoch 100/350, Train Loss: 0.0002866835965493743, Train Accuracy: 79.82%\n",
      "Epoch 101/350, Train Loss: 0.00028224203265195225, Train Accuracy: 80.09%\n",
      "Epoch 102/350, Train Loss: 0.0002782904775786141, Train Accuracy: 80.45%\n",
      "Epoch 103/350, Train Loss: 0.00027513630669960716, Train Accuracy: 80.36%\n",
      "Epoch 104/350, Train Loss: 0.00027030970241162733, Train Accuracy: 80.99%\n",
      "Epoch 105/350, Train Loss: 0.00026464047399484735, Train Accuracy: 81.35%\n",
      "Epoch 106/350, Train Loss: 0.00026152548114465724, Train Accuracy: 81.35%\n",
      "Epoch 107/350, Train Loss: 0.00025873223450027647, Train Accuracy: 81.89%\n",
      "Epoch 108/350, Train Loss: 0.0002543103264666417, Train Accuracy: 82.34%\n",
      "Epoch 109/350, Train Loss: 0.0002520590409543618, Train Accuracy: 81.80%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.00858108223260705, Test Accuracy: 32.34%\n",
      "Epoch 110/350, Train Loss: 0.00024846549682056306, Train Accuracy: 82.43%\n",
      "Epoch 111/350, Train Loss: 0.00024477293599474024, Train Accuracy: 82.16%\n",
      "Epoch 112/350, Train Loss: 0.00024147765813573315, Train Accuracy: 82.70%\n",
      "Epoch 113/350, Train Loss: 0.00023910716222005113, Train Accuracy: 83.06%\n",
      "Epoch 114/350, Train Loss: 0.00023497247807400395, Train Accuracy: 82.97%\n",
      "Epoch 115/350, Train Loss: 0.00023264955088030202, Train Accuracy: 83.06%\n",
      "Epoch 116/350, Train Loss: 0.00023027743727614827, Train Accuracy: 83.42%\n",
      "Epoch 117/350, Train Loss: 0.00022875100649213882, Train Accuracy: 84.14%\n",
      "Epoch 118/350, Train Loss: 0.00022552607042289807, Train Accuracy: 84.23%\n",
      "Epoch 119/350, Train Loss: 0.00022351814067818234, Train Accuracy: 84.14%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008494661325623385, Test Accuracy: 34.05%\n",
      "Epoch 120/350, Train Loss: 0.00022188160892454758, Train Accuracy: 83.96%\n",
      "Epoch 121/350, Train Loss: 0.0002187975515742285, Train Accuracy: 84.41%\n",
      "Epoch 122/350, Train Loss: 0.0002177113291947249, Train Accuracy: 84.77%\n",
      "Epoch 123/350, Train Loss: 0.0002142694280838566, Train Accuracy: 84.77%\n",
      "Epoch 124/350, Train Loss: 0.00021179941023488492, Train Accuracy: 85.50%\n",
      "Epoch 125/350, Train Loss: 0.00020987008096802744, Train Accuracy: 85.14%\n",
      "Epoch 126/350, Train Loss: 0.00020734960007041885, Train Accuracy: 85.41%\n",
      "Epoch 127/350, Train Loss: 0.00020405430829366694, Train Accuracy: 85.77%\n",
      "Epoch 128/350, Train Loss: 0.00020137215273757225, Train Accuracy: 86.13%\n",
      "Epoch 129/350, Train Loss: 0.0001998970291589195, Train Accuracy: 86.13%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008228783744540898, Test Accuracy: 36.58%\n",
      "Epoch 130/350, Train Loss: 0.0001978139952751186, Train Accuracy: 86.40%\n",
      "Epoch 131/350, Train Loss: 0.00019520767065271276, Train Accuracy: 86.58%\n",
      "Epoch 132/350, Train Loss: 0.00019402734669923615, Train Accuracy: 86.13%\n",
      "Epoch 133/350, Train Loss: 0.0001922944248037171, Train Accuracy: 86.76%\n",
      "Epoch 134/350, Train Loss: 0.00019350592438270632, Train Accuracy: 85.95%\n",
      "Epoch 135/350, Train Loss: 0.00019391584973526538, Train Accuracy: 86.13%\n",
      "Epoch 136/350, Train Loss: 0.00019561351240593309, Train Accuracy: 85.86%\n",
      "Epoch 137/350, Train Loss: 0.00019493197107641684, Train Accuracy: 86.13%\n",
      "Epoch 138/350, Train Loss: 0.00019417225557702794, Train Accuracy: 86.31%\n",
      "Epoch 139/350, Train Loss: 0.00019282876512432696, Train Accuracy: 86.49%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.008006585068356798, Test Accuracy: 38.83%\n",
      "Epoch 140/350, Train Loss: 0.00019248069099095125, Train Accuracy: 87.03%\n",
      "Epoch 141/350, Train Loss: 0.00019148466630759705, Train Accuracy: 87.03%\n",
      "Epoch 142/350, Train Loss: 0.0001903735073388447, Train Accuracy: 87.48%\n",
      "Epoch 143/350, Train Loss: 0.0001885961130971069, Train Accuracy: 87.48%\n",
      "Epoch 144/350, Train Loss: 0.00018764546172051177, Train Accuracy: 87.39%\n",
      "Epoch 145/350, Train Loss: 0.00018595101617614808, Train Accuracy: 87.30%\n",
      "Epoch 146/350, Train Loss: 0.0001854057840448511, Train Accuracy: 87.57%\n",
      "Epoch 147/350, Train Loss: 0.000182959907808062, Train Accuracy: 87.75%\n",
      "Epoch 148/350, Train Loss: 0.0001818101568826113, Train Accuracy: 87.48%\n",
      "Epoch 149/350, Train Loss: 0.0001798651903902907, Train Accuracy: 87.75%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.007611957257997097, Test Accuracy: 41.98%\n",
      "Epoch 150/350, Train Loss: 0.00017925720170024252, Train Accuracy: 87.93%\n",
      "Epoch 151/350, Train Loss: 0.00017620393253408068, Train Accuracy: 87.93%\n",
      "Epoch 152/350, Train Loss: 0.00017362262569099499, Train Accuracy: 87.93%\n",
      "Epoch 153/350, Train Loss: 0.00016997218656827372, Train Accuracy: 88.38%\n",
      "Epoch 154/350, Train Loss: 0.00016770164794585097, Train Accuracy: 88.65%\n",
      "Epoch 155/350, Train Loss: 0.00016427539155300055, Train Accuracy: 88.92%\n",
      "Epoch 156/350, Train Loss: 0.000160041796648555, Train Accuracy: 89.10%\n",
      "Epoch 157/350, Train Loss: 0.00015810415542781017, Train Accuracy: 89.01%\n",
      "Epoch 158/350, Train Loss: 0.00015341074882867594, Train Accuracy: 89.91%\n",
      "Epoch 159/350, Train Loss: 0.00015129576924111066, Train Accuracy: 90.09%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.00731929397117406, Test Accuracy: 44.23%\n",
      "Epoch 160/350, Train Loss: 0.00014779689988564734, Train Accuracy: 90.45%\n",
      "Epoch 161/350, Train Loss: 0.00014499954330874577, Train Accuracy: 90.18%\n",
      "Epoch 162/350, Train Loss: 0.00014214150397540794, Train Accuracy: 90.45%\n",
      "Epoch 163/350, Train Loss: 0.00014010135501659355, Train Accuracy: 90.36%\n",
      "Epoch 164/350, Train Loss: 0.00013784504520885732, Train Accuracy: 90.54%\n",
      "Epoch 165/350, Train Loss: 0.0001357358644675709, Train Accuracy: 90.81%\n",
      "Epoch 166/350, Train Loss: 0.00013414875777154642, Train Accuracy: 91.26%\n",
      "Epoch 167/350, Train Loss: 0.00013161707109687754, Train Accuracy: 91.17%\n",
      "Epoch 168/350, Train Loss: 0.00013009848847466427, Train Accuracy: 91.44%\n",
      "Epoch 169/350, Train Loss: 0.00012768194429520546, Train Accuracy: 92.07%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.007110942913580095, Test Accuracy: 46.94%\n",
      "Epoch 170/350, Train Loss: 0.00012550505749506566, Train Accuracy: 92.25%\n",
      "Epoch 171/350, Train Loss: 0.00012411509256904284, Train Accuracy: 92.61%\n",
      "Epoch 172/350, Train Loss: 0.00012094966245405385, Train Accuracy: 92.88%\n",
      "Epoch 173/350, Train Loss: 0.0001191874277111411, Train Accuracy: 93.24%\n",
      "Epoch 174/350, Train Loss: 0.0001163336825524124, Train Accuracy: 93.33%\n",
      "Epoch 175/350, Train Loss: 0.00011500540531164724, Train Accuracy: 93.51%\n",
      "Epoch 176/350, Train Loss: 0.00011317668718055885, Train Accuracy: 93.96%\n",
      "Epoch 177/350, Train Loss: 0.00011102029972280237, Train Accuracy: 93.69%\n",
      "Epoch 178/350, Train Loss: 0.0001086804332645878, Train Accuracy: 94.14%\n",
      "Epoch 179/350, Train Loss: 0.0001062081665840619, Train Accuracy: 94.23%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0069413314111433574, Test Accuracy: 48.20%\n",
      "Epoch 180/350, Train Loss: 0.00010407415407587518, Train Accuracy: 94.41%\n",
      "Epoch 181/350, Train Loss: 0.00010232588328551785, Train Accuracy: 94.41%\n",
      "Epoch 182/350, Train Loss: 0.00010040494910868781, Train Accuracy: 94.77%\n",
      "Epoch 183/350, Train Loss: 9.807407417228729e-05, Train Accuracy: 95.14%\n",
      "Epoch 184/350, Train Loss: 9.653287030122775e-05, Train Accuracy: 95.32%\n",
      "Epoch 185/350, Train Loss: 9.439114859395815e-05, Train Accuracy: 95.50%\n",
      "Epoch 186/350, Train Loss: 9.204136625763713e-05, Train Accuracy: 95.68%\n",
      "Epoch 187/350, Train Loss: 8.936201778768409e-05, Train Accuracy: 95.59%\n",
      "Epoch 188/350, Train Loss: 8.761631198000213e-05, Train Accuracy: 95.95%\n",
      "Epoch 189/350, Train Loss: 8.589696196040122e-05, Train Accuracy: 95.95%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.006770531575328164, Test Accuracy: 50.45%\n",
      "Epoch 190/350, Train Loss: 8.46563959429486e-05, Train Accuracy: 95.95%\n",
      "Epoch 191/350, Train Loss: 8.344130221106699e-05, Train Accuracy: 95.86%\n",
      "Epoch 192/350, Train Loss: 8.246242832986852e-05, Train Accuracy: 95.86%\n",
      "Epoch 193/350, Train Loss: 8.129793214656725e-05, Train Accuracy: 95.86%\n",
      "Epoch 194/350, Train Loss: 7.976319739915522e-05, Train Accuracy: 95.95%\n",
      "Epoch 195/350, Train Loss: 7.88094684745315e-05, Train Accuracy: 96.04%\n",
      "Epoch 196/350, Train Loss: 7.745592142190057e-05, Train Accuracy: 96.04%\n",
      "Epoch 197/350, Train Loss: 7.5806755267504e-05, Train Accuracy: 96.04%\n",
      "Epoch 198/350, Train Loss: 7.498542268349244e-05, Train Accuracy: 96.22%\n",
      "Epoch 199/350, Train Loss: 7.352264259958555e-05, Train Accuracy: 96.22%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.006603747171586773, Test Accuracy: 51.98%\n",
      "Epoch 200/350, Train Loss: 7.257160441723624e-05, Train Accuracy: 96.40%\n",
      "Epoch 201/350, Train Loss: 7.172596640388161e-05, Train Accuracy: 96.58%\n",
      "Epoch 202/350, Train Loss: 7.023946091191217e-05, Train Accuracy: 96.67%\n",
      "Epoch 203/350, Train Loss: 6.952037391015651e-05, Train Accuracy: 96.67%\n",
      "Epoch 204/350, Train Loss: 6.822715742304127e-05, Train Accuracy: 96.94%\n",
      "Epoch 205/350, Train Loss: 6.77660204540682e-05, Train Accuracy: 96.94%\n",
      "Epoch 206/350, Train Loss: 6.693133220167181e-05, Train Accuracy: 96.85%\n",
      "Epoch 207/350, Train Loss: 6.57865404869585e-05, Train Accuracy: 97.03%\n",
      "Epoch 208/350, Train Loss: 6.47248911985024e-05, Train Accuracy: 97.12%\n",
      "Epoch 209/350, Train Loss: 6.408760418698234e-05, Train Accuracy: 97.21%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0063089902656597785, Test Accuracy: 54.05%\n",
      "Epoch 210/350, Train Loss: 6.317262120142889e-05, Train Accuracy: 97.21%\n",
      "Epoch 211/350, Train Loss: 6.222182809314729e-05, Train Accuracy: 97.21%\n",
      "Epoch 212/350, Train Loss: 6.038658181821643e-05, Train Accuracy: 97.30%\n",
      "Epoch 213/350, Train Loss: 5.855946109890372e-05, Train Accuracy: 97.39%\n",
      "Epoch 214/350, Train Loss: 5.773391576827366e-05, Train Accuracy: 97.48%\n",
      "Epoch 215/350, Train Loss: 5.614671722193538e-05, Train Accuracy: 97.48%\n",
      "Epoch 216/350, Train Loss: 5.487993618886555e-05, Train Accuracy: 97.66%\n",
      "Epoch 217/350, Train Loss: 5.353939760661246e-05, Train Accuracy: 97.84%\n",
      "Epoch 218/350, Train Loss: 5.224263413636109e-05, Train Accuracy: 97.93%\n",
      "Epoch 219/350, Train Loss: 5.10092988609342e-05, Train Accuracy: 97.84%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.006067290137798012, Test Accuracy: 55.86%\n",
      "Epoch 220/350, Train Loss: 4.987547258194819e-05, Train Accuracy: 98.02%\n",
      "Epoch 221/350, Train Loss: 4.8971298882170115e-05, Train Accuracy: 98.11%\n",
      "Epoch 222/350, Train Loss: 4.8209786534200866e-05, Train Accuracy: 98.11%\n",
      "Epoch 223/350, Train Loss: 4.712911662413439e-05, Train Accuracy: 98.20%\n",
      "Epoch 224/350, Train Loss: 4.604785522914744e-05, Train Accuracy: 98.29%\n",
      "Epoch 225/350, Train Loss: 4.5822676778391964e-05, Train Accuracy: 98.38%\n",
      "Epoch 226/350, Train Loss: 4.4680034890842256e-05, Train Accuracy: 98.47%\n",
      "Epoch 227/350, Train Loss: 4.390653744340426e-05, Train Accuracy: 98.47%\n",
      "Epoch 228/350, Train Loss: 4.301269818867854e-05, Train Accuracy: 98.56%\n",
      "Epoch 229/350, Train Loss: 4.221056045600542e-05, Train Accuracy: 98.74%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.005836950615333047, Test Accuracy: 57.66%\n",
      "Epoch 230/350, Train Loss: 4.174539046291661e-05, Train Accuracy: 98.74%\n",
      "Epoch 231/350, Train Loss: 4.115468102909491e-05, Train Accuracy: 98.83%\n",
      "Epoch 232/350, Train Loss: 4.025679641007227e-05, Train Accuracy: 98.83%\n",
      "Epoch 233/350, Train Loss: 3.993002773079603e-05, Train Accuracy: 98.83%\n",
      "Epoch 234/350, Train Loss: 3.969010094618266e-05, Train Accuracy: 98.83%\n",
      "Epoch 235/350, Train Loss: 3.862930719720713e-05, Train Accuracy: 98.83%\n",
      "Epoch 236/350, Train Loss: 3.808327977620981e-05, Train Accuracy: 98.83%\n",
      "Epoch 237/350, Train Loss: 3.726364876144544e-05, Train Accuracy: 98.83%\n",
      "Epoch 238/350, Train Loss: 3.667759268790896e-05, Train Accuracy: 98.83%\n",
      "Epoch 239/350, Train Loss: 3.619896102544447e-05, Train Accuracy: 98.83%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.005505596094286016, Test Accuracy: 60.09%\n",
      "Epoch 240/350, Train Loss: 3.590275926238045e-05, Train Accuracy: 98.92%\n",
      "Epoch 241/350, Train Loss: 3.5351207528996606e-05, Train Accuracy: 98.83%\n",
      "Epoch 242/350, Train Loss: 3.4990782739557376e-05, Train Accuracy: 98.83%\n",
      "Epoch 243/350, Train Loss: 3.493697490819693e-05, Train Accuracy: 98.83%\n",
      "Epoch 244/350, Train Loss: 3.4636158350066e-05, Train Accuracy: 98.74%\n",
      "Epoch 245/350, Train Loss: 3.464286282656403e-05, Train Accuracy: 98.74%\n",
      "Epoch 246/350, Train Loss: 3.515514028683172e-05, Train Accuracy: 98.56%\n",
      "Epoch 247/350, Train Loss: 3.575000252964132e-05, Train Accuracy: 98.65%\n",
      "Epoch 248/350, Train Loss: 3.663927394283867e-05, Train Accuracy: 98.56%\n",
      "Epoch 249/350, Train Loss: 3.712401990041359e-05, Train Accuracy: 98.47%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.005208395194367408, Test Accuracy: 61.71%\n",
      "Epoch 250/350, Train Loss: 3.841097602594112e-05, Train Accuracy: 98.29%\n",
      "Epoch 251/350, Train Loss: 3.966203531882341e-05, Train Accuracy: 98.38%\n",
      "Epoch 252/350, Train Loss: 4.029471621868473e-05, Train Accuracy: 98.20%\n",
      "Epoch 253/350, Train Loss: 4.0632731553075705e-05, Train Accuracy: 98.20%\n",
      "Epoch 254/350, Train Loss: 4.112876234451613e-05, Train Accuracy: 98.20%\n",
      "Epoch 255/350, Train Loss: 4.097040085424764e-05, Train Accuracy: 98.20%\n",
      "Epoch 256/350, Train Loss: 4.109955062891312e-05, Train Accuracy: 98.29%\n",
      "Epoch 257/350, Train Loss: 4.029337458725929e-05, Train Accuracy: 98.29%\n",
      "Epoch 258/350, Train Loss: 4.085794580904113e-05, Train Accuracy: 98.29%\n",
      "Epoch 259/350, Train Loss: 4.0484531152633575e-05, Train Accuracy: 98.11%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.004986291080964531, Test Accuracy: 64.68%\n",
      "Epoch 260/350, Train Loss: 4.013806232112132e-05, Train Accuracy: 98.29%\n",
      "Epoch 261/350, Train Loss: 4.011602294977849e-05, Train Accuracy: 98.20%\n",
      "Epoch 262/350, Train Loss: 4.001316877576915e-05, Train Accuracy: 98.20%\n",
      "Epoch 263/350, Train Loss: 3.9631317724401964e-05, Train Accuracy: 98.11%\n",
      "Epoch 264/350, Train Loss: 3.9186784436395e-05, Train Accuracy: 98.20%\n",
      "Epoch 265/350, Train Loss: 3.942675000188115e-05, Train Accuracy: 98.11%\n",
      "Epoch 266/350, Train Loss: 3.975782746453405e-05, Train Accuracy: 98.11%\n",
      "Epoch 267/350, Train Loss: 3.951853646229025e-05, Train Accuracy: 98.20%\n",
      "Epoch 268/350, Train Loss: 3.986120650583587e-05, Train Accuracy: 98.11%\n",
      "Epoch 269/350, Train Loss: 3.9533194671738144e-05, Train Accuracy: 98.20%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.004682409343418568, Test Accuracy: 67.21%\n",
      "Epoch 270/350, Train Loss: 3.941130821362849e-05, Train Accuracy: 98.29%\n",
      "Epoch 271/350, Train Loss: 3.9548716165764185e-05, Train Accuracy: 98.20%\n",
      "Epoch 272/350, Train Loss: 3.931529603405792e-05, Train Accuracy: 98.29%\n",
      "Epoch 273/350, Train Loss: 3.953209729758862e-05, Train Accuracy: 98.11%\n",
      "Epoch 274/350, Train Loss: 3.908379424602636e-05, Train Accuracy: 98.29%\n",
      "Epoch 275/350, Train Loss: 3.917168937622282e-05, Train Accuracy: 98.20%\n",
      "Epoch 276/350, Train Loss: 3.906319630253648e-05, Train Accuracy: 98.20%\n",
      "Epoch 277/350, Train Loss: 3.878641031569176e-05, Train Accuracy: 98.20%\n",
      "Epoch 278/350, Train Loss: 3.8353584835033005e-05, Train Accuracy: 98.20%\n",
      "Epoch 279/350, Train Loss: 3.8561168128650195e-05, Train Accuracy: 98.20%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.004338510658960213, Test Accuracy: 69.55%\n",
      "Epoch 280/350, Train Loss: 3.846010879094354e-05, Train Accuracy: 98.20%\n",
      "Epoch 281/350, Train Loss: 3.824860722877962e-05, Train Accuracy: 98.20%\n",
      "Epoch 282/350, Train Loss: 3.827416867567705e-05, Train Accuracy: 98.20%\n",
      "Epoch 283/350, Train Loss: 3.8354231001592494e-05, Train Accuracy: 98.20%\n",
      "Epoch 284/350, Train Loss: 3.805105566054368e-05, Train Accuracy: 98.20%\n",
      "Epoch 285/350, Train Loss: 3.774089944219821e-05, Train Accuracy: 98.20%\n",
      "Epoch 286/350, Train Loss: 3.734981644133131e-05, Train Accuracy: 98.20%\n",
      "Epoch 287/350, Train Loss: 3.728511241102031e-05, Train Accuracy: 98.20%\n",
      "Epoch 288/350, Train Loss: 3.755446485806582e-05, Train Accuracy: 98.20%\n",
      "Epoch 289/350, Train Loss: 3.6690727786882195e-05, Train Accuracy: 98.20%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0039047885308760187, Test Accuracy: 72.34%\n",
      "Epoch 290/350, Train Loss: 3.711595561144086e-05, Train Accuracy: 98.20%\n",
      "Epoch 291/350, Train Loss: 3.6291984561611935e-05, Train Accuracy: 98.20%\n",
      "Epoch 292/350, Train Loss: 3.651739340704144e-05, Train Accuracy: 98.20%\n",
      "Epoch 293/350, Train Loss: 3.637057605030087e-05, Train Accuracy: 98.20%\n",
      "Epoch 294/350, Train Loss: 3.497313987618253e-05, Train Accuracy: 98.20%\n",
      "Epoch 295/350, Train Loss: 3.4888717680929044e-05, Train Accuracy: 98.29%\n",
      "Epoch 296/350, Train Loss: 3.5026112975812184e-05, Train Accuracy: 98.29%\n",
      "Epoch 297/350, Train Loss: 3.443832204757349e-05, Train Accuracy: 98.20%\n",
      "Epoch 298/350, Train Loss: 3.4002404004286043e-05, Train Accuracy: 98.29%\n",
      "Epoch 299/350, Train Loss: 3.3732047056268985e-05, Train Accuracy: 98.29%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0034552270278379437, Test Accuracy: 74.95%\n",
      "Epoch 300/350, Train Loss: 3.3292702337876475e-05, Train Accuracy: 98.29%\n",
      "Epoch 301/350, Train Loss: 3.249627161426801e-05, Train Accuracy: 98.38%\n",
      "Epoch 302/350, Train Loss: 3.215253649818083e-05, Train Accuracy: 98.38%\n",
      "Epoch 303/350, Train Loss: 3.1766692112990455e-05, Train Accuracy: 98.38%\n",
      "Epoch 304/350, Train Loss: 3.138258365191191e-05, Train Accuracy: 98.38%\n",
      "Epoch 305/350, Train Loss: 3.083026443656007e-05, Train Accuracy: 98.47%\n",
      "Epoch 306/350, Train Loss: 3.08048397518441e-05, Train Accuracy: 98.47%\n",
      "Epoch 307/350, Train Loss: 3.00494044634091e-05, Train Accuracy: 98.56%\n",
      "Epoch 308/350, Train Loss: 2.972581321907519e-05, Train Accuracy: 98.47%\n",
      "Epoch 309/350, Train Loss: 2.9179022565668565e-05, Train Accuracy: 98.65%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.002900591210382694, Test Accuracy: 78.56%\n",
      "Epoch 310/350, Train Loss: 2.8832451193983408e-05, Train Accuracy: 98.65%\n",
      "Epoch 311/350, Train Loss: 2.8606909779406437e-05, Train Accuracy: 98.74%\n",
      "Epoch 312/350, Train Loss: 2.8193261776497268e-05, Train Accuracy: 98.74%\n",
      "Epoch 313/350, Train Loss: 2.7877864649438122e-05, Train Accuracy: 98.74%\n",
      "Epoch 314/350, Train Loss: 2.7597345550067188e-05, Train Accuracy: 98.83%\n",
      "Epoch 315/350, Train Loss: 2.7223643002618364e-05, Train Accuracy: 98.83%\n",
      "Epoch 316/350, Train Loss: 2.6723043908994697e-05, Train Accuracy: 98.83%\n",
      "Epoch 317/350, Train Loss: 2.622185261286762e-05, Train Accuracy: 98.83%\n",
      "Epoch 318/350, Train Loss: 2.5284150022206657e-05, Train Accuracy: 98.83%\n",
      "Epoch 319/350, Train Loss: 2.4627889811076417e-05, Train Accuracy: 98.92%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.002383986307352755, Test Accuracy: 81.98%\n",
      "Epoch 320/350, Train Loss: 2.402516677167844e-05, Train Accuracy: 98.92%\n",
      "Epoch 321/350, Train Loss: 2.3140765298636517e-05, Train Accuracy: 99.01%\n",
      "Epoch 322/350, Train Loss: 2.2021440168621778e-05, Train Accuracy: 99.01%\n",
      "Epoch 323/350, Train Loss: 2.046058113650735e-05, Train Accuracy: 99.01%\n",
      "Epoch 324/350, Train Loss: 1.8832558593270172e-05, Train Accuracy: 99.46%\n",
      "Epoch 325/350, Train Loss: 1.670178035738367e-05, Train Accuracy: 99.46%\n",
      "Epoch 326/350, Train Loss: 1.4909238773069437e-05, Train Accuracy: 99.82%\n",
      "Epoch 327/350, Train Loss: 1.3430115426513562e-05, Train Accuracy: 99.82%\n",
      "Epoch 328/350, Train Loss: 1.2970597675077921e-05, Train Accuracy: 100.00%\n",
      "Epoch 329/350, Train Loss: 1.2611738363105886e-05, Train Accuracy: 100.00%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0018774985677952806, Test Accuracy: 86.22%\n",
      "Epoch 330/350, Train Loss: 1.239721419122983e-05, Train Accuracy: 100.00%\n",
      "Epoch 331/350, Train Loss: 1.2152306174192043e-05, Train Accuracy: 100.00%\n",
      "Epoch 332/350, Train Loss: 1.1965550454064753e-05, Train Accuracy: 100.00%\n",
      "Epoch 333/350, Train Loss: 1.1804819911738292e-05, Train Accuracy: 100.00%\n",
      "Epoch 334/350, Train Loss: 1.1643071052919242e-05, Train Accuracy: 100.00%\n",
      "Epoch 335/350, Train Loss: 1.148368596716099e-05, Train Accuracy: 100.00%\n",
      "Epoch 336/350, Train Loss: 1.1380235978745463e-05, Train Accuracy: 100.00%\n",
      "Epoch 337/350, Train Loss: 1.135122562044509e-05, Train Accuracy: 100.00%\n",
      "Epoch 338/350, Train Loss: 1.1183688958607317e-05, Train Accuracy: 100.00%\n",
      "Epoch 339/350, Train Loss: 1.1151254882526496e-05, Train Accuracy: 100.00%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0015791746338345246, Test Accuracy: 87.93%\n",
      "Epoch 340/350, Train Loss: 1.1121548285918328e-05, Train Accuracy: 100.00%\n",
      "Epoch 341/350, Train Loss: 1.1206122655729155e-05, Train Accuracy: 100.00%\n",
      "Epoch 342/350, Train Loss: 1.120113509037759e-05, Train Accuracy: 100.00%\n",
      "Epoch 343/350, Train Loss: 1.1317135360384927e-05, Train Accuracy: 99.91%\n",
      "Epoch 344/350, Train Loss: 1.1268894423565463e-05, Train Accuracy: 99.91%\n",
      "Epoch 345/350, Train Loss: 1.1346865858848553e-05, Train Accuracy: 99.82%\n",
      "Epoch 346/350, Train Loss: 1.1272184618730292e-05, Train Accuracy: 99.82%\n",
      "Epoch 347/350, Train Loss: 1.119971007549951e-05, Train Accuracy: 99.82%\n",
      "Epoch 348/350, Train Loss: 1.1431861262238009e-05, Train Accuracy: 99.73%\n",
      "Epoch 349/350, Train Loss: 1.1712733812763106e-05, Train Accuracy: 99.73%\n",
      "모델 범용성 평가\n",
      "Test Loss: 0.0014032966185046033, Test Accuracy: 90.00%\n",
      "Epoch 350/350, Train Loss: 1.2536429426882135e-05, Train Accuracy: 99.55%\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "nn.train(train_data, train_labels, epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
