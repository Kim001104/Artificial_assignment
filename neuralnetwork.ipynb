{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 위한 데이터셋 마련하기.\n",
    "한글 폰트 총 10개 사용.(굴림,새굴림,휴먼아미체,휴먼엑스포체,한양해서,HY견고딕,HY견명조,HY그래픽,HY울릉도B)\n",
    "한글 데이터셋의 디렉토리에 train이라는 디렉토리를 생성하여 리스트에 있는 단어들을 폰트들과 조합하여 각각의 단어들로 디렉토리를 생성하였고, 최종적으로 각각의 디렉토리에는 각각의 단어별 총 10가지의 폰트들의 bmp파일이 담겨져 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "sentences = [\n",
    "    \"경 기 도 고 양 시 일 산 서 구 탄 현 로 1 3 6 1 1 2 동 1 6 0 1 호\",\n",
    "    \"김 동 현\",\n",
    "    \"김 재 규 이 경 숙 김 효 섭 박 노 쇠\",\n",
    "    \"인 천 광 역 시 연 수 구 아 카 데 미 로 1 1 9\",\n",
    "    \"청 춘 이 는 듣 기 만 하 여 도 가 슴 이 설 레 는 말 이 다 너 의 두 손을 가 슴 에 대 고 물 방 아 같 은 심 장 의 고 동 을 들 어 보 라\",\n",
    "    \"청 춘 의 피 는 끓 는 다 피 에 뛰 노 는 심 장 은 거 선 의 기 관 같 이 힘 있 다 이 것 이 다 인 류 의 역 사 를 꾸 며 내 려 온 동 력 은 바 로 이 것 이 다\",\n",
    "    \"이 성 은 투 명 하 되 얼 음 과 같 으 며 지 혜 는 날 카 로 우 나 갑 속 에 든 칼 이 다 청 춘 의 끓 는 피 가 아 니 더 면 인 간 이 얼 마 나 쓸 쓸 하 랴\",\n",
    "    \"얼 음 에 싸 인 만 물 은 죽 음 이 있 을 뿐 이 다\"\n",
    "]\n",
    "\n",
    "#고유한 글자 추출\n",
    "unique_chars = set()\n",
    "for sentence in sentences:\n",
    "    for char in sentence:\n",
    "        # 공백 및 특수문자 제외\n",
    "        if char.strip() and char.isalnum():\n",
    "            unique_chars.add(char)\n",
    "\n",
    "print(f\"총 고유 글자 수: {len(unique_chars)}\")\n",
    "\n",
    "#이미지 저장 디렉토리 설정\n",
    "base_output_dir = \"c:\\\\Users\\\\motus\\\\OneDrive\\\\바탕 화면\\\\인공지능\\\\인공지능 과제2_글자\\\\한글글자데이터\\\\train\"\n",
    "if not os.path.exists(base_output_dir):\n",
    "    os.makedirs(base_output_dir)\n",
    "\n",
    "#폰트 파일 경로 설정\n",
    "#여러 폰트를 리스트로 설정\n",
    "font_paths = [\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\gulim.ttc\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\NGULIM.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HMFMPYUN.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HMKMAMI.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HMFMMUEX.TTC\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\UNI_HSR.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HYGTRE.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HYMJRE.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HYGPRM.TTF\",\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HYWULB.TTF\"\n",
    "]\n",
    "\n",
    "#글자당 이미지 생성 함수\n",
    "def create_char_image(char, font_path, save_dir, img_size=(64, 64), font_size=64): \n",
    "        # 흰색 배경으로 이미지 생성\n",
    "        img = Image.new('L', img_size, color=255)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        font = ImageFont.truetype(font_path, size=font_size)\n",
    "        \n",
    "        # 글자의 크기 계산\n",
    "        width, height = draw.textsize(char, font=font)\n",
    "        \n",
    "        # 글자를 중앙에 배치\n",
    "        position = ((img_size[0]-width)/2, (img_size[1]-height)/2)\n",
    "        draw.text(position, char, fill=0, font=font)\n",
    "        \n",
    "        # 폰트 이름 추출 (파일명에서 확장자 제거)\n",
    "        font_name = os.path.splitext(os.path.basename(font_path))[0]\n",
    "        \n",
    "        # 이미지 파일명에 글자와 폰트 이름 포함\n",
    "        img_filename = f\"{char}_{font_name}.bmp\"\n",
    "        img_path = os.path.join(save_dir, img_filename)\n",
    "        img.save(img_path)\n",
    "        \n",
    "#모든 고유 글자에 대해 이미지 생성\n",
    "for char in unique_chars:\n",
    "    # 글자별 디렉토리 경로 설정\n",
    "    char_dir = os.path.join(base_output_dir, char)\n",
    "    if not os.path.exists(char_dir):\n",
    "        os.makedirs(char_dir)\n",
    "    \n",
    "    # 각폰트별로 이미지 생성 및 저장\n",
    "    for font_path in font_paths:\n",
    "        create_char_image(char, font_path, char_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test라는 디렉토리 안에 각기 다른 폰트 3가지를 통하여서 test 데이터 셋을 만듦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# 1. 주어진 문장들\n",
    "sentences = [\n",
    "    \"경 기 도 고 양 시 일 산 서 구 탄 현 로 1 3 6 1 1 2 동 1 6 0 1 호\",\n",
    "    \"김 동 현\",\n",
    "    \"김 재 규 이 경 숙 김 효 섭 박 노 쇠\",\n",
    "    \"인 천 광 역 시 연 수 구 아 카 데 미 로 1 1 9\",\n",
    "    \"청 춘 이 는 듣 기 만 하 여 도 가 슴 이 설 레 는 말 이 다 너 의 두 손을 가 슴 에 대 고 물 방 아 같 은 심 장 의 고 동 을 들 어 보 라\",\n",
    "    \"청 춘 의 피 는 끓 는 다 피 에 뛰 노 는 심 장 은 거 선 의 기 관 같 이 힘 있 다 이 것 이 다 인 류 의 역 사 를 꾸 며 내 려 온 동 력 은 바 로 이 것 이 다\",\n",
    "    \"이 성 은 투 명 하 되 얼 음 과 같 으 며 지 혜 는 날 카 로 우 나 갑 속 에 든 칼 이 다 청 춘 의 끓 는 피 가 아 니 더 면 인 간 이 얼 마 나 쓸 쓸 하 랴\",\n",
    "    \"얼 음 에 싸 인 만 물 은 죽 음 이 있 을 뿐 이 다\"\n",
    "]\n",
    "\n",
    "# 2. 고유한 글자 추출\n",
    "unique_chars = set()\n",
    "for sentence in sentences:\n",
    "    for char in sentence:\n",
    "        if char.strip() and char.isalnum():  # 공백 및 특수문자 제외\n",
    "            unique_chars.add(char)\n",
    "\n",
    "print(f\"총 고유 글자 수: {len(unique_chars)}\")\n",
    "\n",
    "# 3. 이미지 저장 디렉토리 설정\n",
    "base_output_dir = \"c:\\\\Users\\\\motus\\\\OneDrive\\\\바탕 화면\\\\인공지능\\\\인공지능 과제2_글자\\\\한글글자데이터\\\\test\"\n",
    "if not os.path.exists(base_output_dir):\n",
    "    os.makedirs(base_output_dir)\n",
    "\n",
    "# 4. 폰트 경로 설정\n",
    "font_paths = [\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\ahn_l.ttf\",    # 안상수2006가는보통\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HMFMOLD.TTF\", # 휴먼옛체\n",
    "    \"C:\\\\Windows\\\\Fonts\\\\HMFMPYUN.TTF\"  # 휴먼편지체\n",
    "]\n",
    "\n",
    "# 5. 이미지 생성 및 변형 함수\n",
    "def create_and_augment_image(char, font_path, output_path):\n",
    "    # 글자 이미지 생성\n",
    "    font = ImageFont.truetype(font_path, 64)  # 폰트 크기 설정\n",
    "    image = Image.new(\"L\", (64, 64), \"white\")  # 흰 배경의 64x64 이미지 생성\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    text_width, text_height = draw.textsize(char, font=font)\n",
    "    draw.text(((64 - text_width) // 2, (64 - text_height) // 2), char, font=font, fill=\"black\")\n",
    "\n",
    "    # 변형 적용 함수\n",
    "    image = augment_image(image)\n",
    "    \n",
    "    # 이미지 저장\n",
    "    image.save(output_path, format=\"bmp\")\n",
    "\n",
    "# 6. 변형 함수 (가로/세로 노이즈 추가)\n",
    "def augment_image(image):\n",
    "    \n",
    "    transformations = \"add_noise\"\n",
    "\n",
    "    for transform in transformations:\n",
    "        if transform == \"add_noise\":\n",
    "            image_cv = np.array(image)\n",
    "            noise = np.zeros_like(image_cv)\n",
    "\n",
    "            # 가로 또는 세로 방향 중 하나 선택\n",
    "            noise_type = random.choice([\"horizontal\", \"vertical\"])\n",
    "\n",
    "            if noise_type == \"horizontal\":\n",
    "                for i in range(0, image_cv.shape[0], 5):  # 몇몇 행에만 노이즈 추가\n",
    "                    noise[i, :] = np.random.normal(0, 10, image_cv.shape[1])\n",
    "            elif noise_type == \"vertical\":\n",
    "                for j in range(0, image_cv.shape[1], 5):  # 몇몇 열에만 노이즈 추가\n",
    "                    noise[:, j] = np.random.normal(0, 10, image_cv.shape[0])\n",
    "\n",
    "            image_cv = cv2.add(image_cv, noise.astype(np.uint8))\n",
    "            image = Image.fromarray(image_cv)\n",
    "\n",
    "    return image\n",
    "\n",
    "# 7. 각 글자마다 디렉토리 생성 및 이미지 저장\n",
    "for char in unique_chars:\n",
    "    char_dir = os.path.join(base_output_dir, char)\n",
    "    os.makedirs(char_dir, exist_ok=True)\n",
    "\n",
    "    # 파일 번호 초기화\n",
    "    file_index = 1\n",
    "\n",
    "    for font_path in font_paths:\n",
    "        output_path = os.path.join(char_dir, f\"{file_index}.bmp\")\n",
    "        create_and_augment_image(char, font_path, output_path)\n",
    "        \n",
    "        # 인덱스를 증가시켜 다음 이미지 파일 이름으로 사용\n",
    "        file_index += 1\n",
    "\n",
    "print(\"모든 글자 이미지 생성 및 변형 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 각 글자들을 통해서 bmp파일을 만들었으니 컴퓨터가 이 픽셀 데이터를 읽기 쉽게 각각의 csv파일을 만듦.\n",
    "합치고 병합하여 train과 test 디렉토리에 각각 merger_data셋을 만듦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# BMP 파일이 저장된 최상위 폴더 경로 설정 (train 폴더 or test 폴더)\n",
    "train_folder_path = \"C:\\\\Users\\\\motus\\\\OneDrive\\\\바탕 화면\\\\인공지능\\\\인공지능 과제2_글자\\\\한글글자데이터\\\\test\"\n",
    "merge_data = []\n",
    "\n",
    "# train, test 폴더 내 하위 폴더를 재귀적으로 탐색\n",
    "for root, dirs, files in os.walk(train_folder_path):\n",
    "    # BMP 파일 중 숫자로 된 이름만 정렬하여 가져오기\n",
    "    bmp_files = sorted(\n",
    "        [f for f in files if f.endswith(\".bmp\") and os.path.splitext(f)[0].isdigit()],\n",
    "        key=lambda x: int(os.path.splitext(x)[0])\n",
    "    )\n",
    "    \n",
    "    if bmp_files:  # 숫자로 된 BMP 파일이 있는 폴더만 처리\n",
    "        data = []\n",
    "        for filename in bmp_files:\n",
    "            # 파일명에서 레이블 추출 (확장자를 제외한 파일명)\n",
    "            label = os.path.splitext(filename)[0]\n",
    "\n",
    "            # BMP 파일 열기 및 흑백으로 변환\n",
    "            file_path = os.path.join(root, filename)\n",
    "            img = Image.open(file_path).convert('L')  # 흑백 모드로 변환\n",
    "            pixel_data = list(img.getdata())  # 픽셀 데이터를 리스트로 변환\n",
    "\n",
    "            # 파일 이름과 픽셀 데이터를 합쳐서 하나의 행으로 만들기\n",
    "            row = [label] + pixel_data  # 첫 번째 열에 파일 이름(레이블) 추가\n",
    "            data.append(row)\n",
    "\n",
    "        # 데이터프레임 생성 (컬럼 이름 없이 파일 이름과 픽셀 정보만 저장)\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # 현재 폴더 이름을 CSV 파일명으로 설정\n",
    "        folder_name = os.path.basename(root)\n",
    "        csv_file_path = os.path.join(root, f\"{folder_name}.csv\")\n",
    "        df.to_csv(csv_file_path, index=False, header=False)  # 헤더 없이 CSV 파일 저장\n",
    "\n",
    "        print(f\"{csv_file_path} 파일이 성공적으로 생성되었습니다.\")\n",
    "        print(f\"CSV 파일 저장 경로: {csv_file_path}\")\n",
    "\n",
    "\n",
    "#파일 삭제하기 \n",
    "# import os\n",
    "\n",
    "# # BMP 파일이 저장된 최상위 폴더 경로 설정 (train 폴더 or test 폴더)\n",
    "# train_folder_path = \"C:\\\\Users\\\\motus\\\\OneDrive\\\\바탕 화면\\\\인공지능\\\\인공지능 과제2_글자\\\\한글글자데이터\\\\test\"\n",
    "\n",
    "# # train_folder_path 내 하위 폴더를 재귀적으로 탐색하며 '.csv' 파일 삭제\n",
    "# for root, dirs, files in os.walk(train_folder_path):\n",
    "#     for file in files:\n",
    "#         # 파일 이름이 '11.bmp'로 끝나는 경우 삭제\n",
    "#         if file == \".csv\":\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             os.remove(file_path)\n",
    "#             print(f\"{file_path} 파일이 삭제되었습니다.\")\n",
    "\n",
    "# 최상위 폴더 경로 설정\n",
    "train_folder_path = \"C:\\\\Users\\\\motus\\\\OneDrive\\\\바탕 화면\\\\인공지능\\\\인공지능 과제2_글자\\\\한글글자데이터\\\\test\"\n",
    "\n",
    "# CSV 파일을 저장할 리스트 초기화\n",
    "csv_files = []\n",
    "\n",
    "# train 폴더 내 모든 하위 폴더와 파일 순회\n",
    "for root, dirs, files in os.walk(train_folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            # CSV 파일의 전체 경로를 리스트에 추가\n",
    "            csv_files.append(os.path.join(root, file))\n",
    "\n",
    "# CSV 파일이 있는지 확인하고 병합\n",
    "if csv_files:\n",
    "    # 각 CSV 파일을 데이터프레임으로 읽어와서 병합\n",
    "    merged_data = pd.concat([pd.read_csv(file, header=None) for file in csv_files], ignore_index=True)\n",
    "\n",
    "    # 병합된 데이터프레임을 하나의 CSV 파일로 저장\n",
    "    merged_csv_file_path = os.path.join(train_folder_path, \"merged_data.csv\")\n",
    "    merged_data.to_csv(merged_csv_file_path, index=False, header=False)\n",
    "\n",
    "    print(f\"모든 CSV 파일이 성공적으로 병합되어 {merged_csv_file_path}에 저장되었습니다.\")\n",
    "else:\n",
    "    print(\"병합할 CSV 파일이 없습니다. test 폴더를 확인해 주세요.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 구성하기\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 4096\n",
    "hidden_nodes = [3000,2000,1000,500,100]\n",
    "output_nodes = 10\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "n=neuralNetwork(input_nodes,hidden_nodes,output_nodes,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m training_data_file:\n\u001b[0;32m      9\u001b[0m         all_values \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m(numpy\u001b[38;5;241m.\u001b[39masfarray(all_values[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.99\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "training_data_file = open(\"C:\\\\Users\\\\motus\\\\OneDrive\\\\바탕 화면\\\\인공지능\\\\인공지능 과제2_글자\\\\한글글자데이터\\\\train\\\\train_data.csv\")\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraining_data_file\u001b[49m:\n\u001b[0;32m      5\u001b[0m         all_values \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m(numpy\u001b[38;5;241m.\u001b[39masfarray(all_values[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.99\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data_file' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for record in training_data_file:\n",
    "        all_values = record.split(',')\n",
    "        inputs =(numpy.asfarray(all_values[1:]) / 255.0 + 0.99) + 0.01\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs,targets)\n",
    "    pass\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
